{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection London runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "The data files associated with this notebook are not included in this repository, largely because they are too large.  If you are interested in further information on this project please do get in touch via Github or Linkedin and I would be very pleased to discuss with you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from time import sleep\n",
    "from random import randint\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap = ListedColormap(sns.color_palette(\"husl\", 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters for scraping\n",
    "user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.183 Safari/537.36\"\n",
    "accept = \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\"\n",
    "encoding = \"gzip, deflate, br\"\n",
    "lang = \"en-GB,en;q=0.9\"\n",
    "# headers = {'user-agent': user_agent}\n",
    "headers = {'accept': accept, 'accept-encoding': encoding, 'accept-language': lang, 'user-agent': user_agent}\n",
    "URL_event_template = \"https://www.parkrun.org.uk/{}/results/eventhistory/\"\n",
    "URL_run_template = 'https://www.parkrun.org.uk/{}/results/weeklyresults/?runSeqNumber={}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
       " 'accept-encoding': 'gzip, deflate, br',\n",
       " 'accept-language': 'en-GB,en;q=0.9',\n",
       " 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.183 Safari/537.36'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'allypally'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load event list\n",
    "london_links = pd.read_csv('london_links.csv')\n",
    "event_names = london_links.event_name\n",
    "event_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(event_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['greenwich', 'tootingcommon', 'bromley', 'barking']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# divide list into tranches for scraping\n",
    "# first event run as test\n",
    "event_group_1 = event_names[0]\n",
    "# second partially scraped only (blocked)\n",
    "event_group_2 = event_names[1]\n",
    "event_group_3 = event_names[2:6]\n",
    "event_group_4 = event_names[6]\n",
    "# group 5 failed 'bromley'\n",
    "event_group_5 = event_names[7]\n",
    "event_group_6 = event_names[8:17] # all ok\n",
    "event_group_7 = event_names[17:]\n",
    "failed_events_7 = ['greenwich', 'tootingcommon']\n",
    "failed_events_all = failed_events_7 + [event_group_5] + [event_group_2]\n",
    "failed_events_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to extract latest run number from event history\n",
    "def find_latest(name):\n",
    "    URL_event_template = \"https://www.parkrun.org.uk/{}/results/eventhistory/\"\n",
    "    r_EH = requests.get(URL_event_template.format(name), headers=headers)\n",
    "    EH_soup = BeautifulSoup(r_EH.text, 'html.parser')\n",
    "    \n",
    "    latest_row = EH_soup.find_all('tr', attrs={'class': \"Results-table-row\"})[0]\n",
    "    latest_run = latest_row.find('td', attrs={'class': \"Results-table-td Results-table-td--position\"}).text\n",
    "    \n",
    "    return latest_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to parse data using Beautiful soup\n",
    "def data_cleaner(r):\n",
    "    event_soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    results_1 = event_soup.find_all('tr', attrs={'class':\"Results-table-row\"})\n",
    "    \n",
    "    date = event_soup.find_all('h3')[0].text.split(\" | #\")[0]\n",
    "    event_no = event_soup.find_all('h3')[0].text.split(\" | #\")[1]\n",
    "    event_name = event_soup.find_all('h1')[0].text\n",
    "    \n",
    "    positions = []\n",
    "    names = []\n",
    "    total_parkruns = []\n",
    "    run_time = []\n",
    "    event_PB = []\n",
    "    clubs = []\n",
    "    age_groups = []\n",
    "    age_grades = []\n",
    "    athlete_no = []\n",
    "    \n",
    "    for result in results_1:\n",
    "        try:\n",
    "            positions.append(result.find(\"td\", attrs={\"class\":\"Results-table-td Results-table-td--position\"}).text)\n",
    "        except:\n",
    "            positions.append(np.nan)\n",
    "        try:\n",
    "            names.append(result.find(\"div\", attrs={\"class\":\"compact\"}).text)\n",
    "        except:\n",
    "            names.append(np.nan)\n",
    "        try:\n",
    "            tp = re.split(r'\\xa0\\n', result.find(\"div\", attrs={\"class\":\"detailed\"}).text)[0]\n",
    "            total_parkruns.append(tp)\n",
    "        except:\n",
    "            total_parkruns.append(np.nan)\n",
    "        try:\n",
    "            time = result.find(\"td\", attrs={\"class\":\"Results-table-td--time\"}).text\n",
    "            if time[-2] == 'r':\n",
    "                run_time.append(re.split(r'First Timer!', time)[0])\n",
    "                event_PB.append(re.split(r'First Timer!', time)[1])\n",
    "            elif time[-2] == 'B':\n",
    "                run_time.append(re.split(r'New PB!', time)[0])\n",
    "                event_PB.append(re.split(r'New PB!', time)[1])            \n",
    "            else:\n",
    "                run_time.append(re.split(r'PB\\xa0', time)[0])\n",
    "                event_PB.append(re.split(r'PB\\xa0', time)[1])\n",
    "        except:\n",
    "            run_time.append(np.nan)\n",
    "            event_PB.append(np.nan)\n",
    "        try:\n",
    "            club = result.find(\"td\", attrs={\"class\":\"Results-table-td Results-table-td--club\"}).text\n",
    "            if \"\\n\" in club or \"\\xa0\" in club:\n",
    "                clubs.append(np.nan)\n",
    "            else:\n",
    "                clubs.append(club)\n",
    "        except:\n",
    "            clubs.append(np.nan)\n",
    "        try:\n",
    "            grading = result.find(\"td\", attrs={\"class\":\"Results-table-td Results-table-td--ageGroup\"}).text\n",
    "            if len(grading) < 5:\n",
    "                age_groups.append(np.nan)\n",
    "                age_grades.append(np.nan)\n",
    "            else:\n",
    "                age_groups.append(grading.split('%')[0][:7])\n",
    "                age_grades.append(grading.split('%')[0][7:])\n",
    "        except:\n",
    "            age_groups.append(np.nan)\n",
    "            age_grades.append(np.nan)\n",
    "        try:\n",
    "            ref = result.find_all(href=True)[0]\n",
    "            athlete_no.append(str(ref).split('\"')[1].split('=')[-1])\n",
    "        except:\n",
    "            athlete_no.append(np.nan)\n",
    "            \n",
    "    df = pd.DataFrame({'event_name': event_name,\n",
    "                       'event_no': event_no,\n",
    "                       'date': date,\n",
    "                       'positions': positions,\n",
    "                       'athlete_no': athlete_no,\n",
    "                       'names': names,\n",
    "                       'total_parkruns': total_parkruns,\n",
    "                       'run_time': run_time,\n",
    "                       'event_PB': event_PB,\n",
    "                       'club': clubs,\n",
    "                       'age_groups': age_groups,\n",
    "                       'age_grades': age_grades})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['greenwich', 'tootingcommon', 'bromley', 'barking']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed_events_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 181/205 [20:41<02:44,  6.86s/it]\n",
      " 80%|████████  | 164/205 [15:32<03:53,  5.69s/it]\n",
      "  8%|▊         | 17/205 [02:41<29:42,  9.48s/it]\n",
      "100%|██████████| 205/205 [20:47<00:00,  6.09s/it]\n"
     ]
    }
   ],
   "source": [
    "event_list = failed_events_all\n",
    "failed_events = []\n",
    "\n",
    "for event in event_list:\n",
    "    \n",
    "    try:\n",
    "        latest_run = int(find_latest(event))\n",
    "        \n",
    "        # define start to get max 2 years data\n",
    "        if latest_run > 204:\n",
    "            latest_run_list = list(range(latest_run+1))\n",
    "            latest_run_start = latest_run_list[-205]\n",
    "        else:\n",
    "            latest_run_start = 1\n",
    "        \n",
    "        URL_run_template = 'https://www.parkrun.org.uk/{}/results/weeklyresults/?runSeqNumber={}'\n",
    "        count = 0\n",
    "        \n",
    "        # keep track of current event name in case of error\n",
    "        working_event = event\n",
    "        \n",
    "        for run in tqdm(list(range(latest_run_start, latest_run+1))):\n",
    "            \n",
    "            # get run data with requets\n",
    "            URL_run = URL_run_template.format(event, run)\n",
    "            r_run = requests.get(URL_run, headers=headers)\n",
    "            \n",
    "            df_run = data_cleaner(r_run)\n",
    "    \n",
    "            if count == 0:\n",
    "                df = df_run\n",
    "            else:\n",
    "                df = df.append(df_run, ignore_index=True)\n",
    "            \n",
    "            count += 1\n",
    "        \n",
    "            # pause to slow scraping\n",
    "            sleep(randint(3,5))\n",
    "    \n",
    "        file_name = 'all_runs_{}.csv'\n",
    "        df.to_csv(file_name.format(event), index=False)\n",
    "        \n",
    "        # extra sleep between events\n",
    "        sleep(randint(60,65))\n",
    "        \n",
    "    except:\n",
    "        failed_events.append(event)\n",
    "        sleep(randint(60,65))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(205, 'barking')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count, working_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['greenwich', 'tootingcommon', 'bromley']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed_events_8 = ['greenwich', 'tootingcommon', 'bromley']\n",
    "failed_events_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_events_7 = ['greenwich', 'tootingcommon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# for failed events try in reverse order, saving after each run\n",
    "# the create 2nd file and start forwards, to converge on bug, saving as all_runs_{event}_2\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 181/205 [15:09<02:00,  5.02s/it]\n",
      " 80%|████████  | 164/205 [15:14<03:48,  5.58s/it]\n",
      "  8%|▊         | 17/205 [01:41<18:45,  5.99s/it]\n"
     ]
    }
   ],
   "source": [
    "event_list = failed_events_8\n",
    "failed_events = []\n",
    "\n",
    "for event in event_list:\n",
    "    \n",
    "    try:\n",
    "        latest_run = int(find_latest(event))\n",
    "        \n",
    "        # define start to get max 2 years data\n",
    "        if latest_run > 204:\n",
    "            latest_run_list = list(range(latest_run+1))\n",
    "            latest_run_start = latest_run_list[-205]\n",
    "        else:\n",
    "            latest_run_start = 1\n",
    "        \n",
    "        URL_run_template = 'https://www.parkrun.org.uk/{}/results/weeklyresults/?runSeqNumber={}'\n",
    "        count = 0\n",
    "        \n",
    "        # keep track of current event name in case of error\n",
    "        working_event = event\n",
    "        \n",
    "        for run in tqdm(list(range(latest_run_start, latest_run+1))):\n",
    "            \n",
    "            # get run data with requets\n",
    "            URL_run = URL_run_template.format(event, run)\n",
    "            r_run = requests.get(URL_run, headers=headers)\n",
    "            \n",
    "            df_run = data_cleaner(r_run)\n",
    "    \n",
    "            if count == 0:\n",
    "                df = df_run\n",
    "            else:\n",
    "                df = df.append(df_run, ignore_index=True)\n",
    "            \n",
    "            count += 1\n",
    "        \n",
    "            # pause to slow scraping\n",
    "            sleep(randint(3,5))\n",
    "    \n",
    "            file_name = 'all_runs_{}_2.csv'\n",
    "            df.to_csv(file_name.format(event), index=False)\n",
    "        \n",
    "        # extra sleep between events\n",
    "        sleep(randint(60,65))\n",
    "        \n",
    "    except:\n",
    "        failed_events.append(event)\n",
    "        sleep(randint(60,65))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# single event scrape below\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 415/415 [32:23<00:00,  4.68s/it]\n"
     ]
    }
   ],
   "source": [
    "# remove comments to test\n",
    "event = event_names[0]\n",
    "latest_run = find_latest(event)\n",
    "URL_run_template = 'https://www.parkrun.org.uk/{}/results/weeklyresults/?runSeqNumber={}'\n",
    "count = 0\n",
    "\n",
    "#for run in tqdm(list(range(1,int(latest_run)+1))):\n",
    "    \n",
    "    # get run data with requets\n",
    "    URL_run = URL_run_template.format(event, run)\n",
    "    #r_run = requests.get(URL_run, headers=headers)\n",
    "    \n",
    "    df_run = data_cleaner(r_run)\n",
    "    \n",
    "    if count == 0:\n",
    "        df = df_run\n",
    "    else:\n",
    "        df = df.append(df_run, ignore_index=True)\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    # pause to slow scraping\n",
    "    sleep(randint(3,5))\n",
    "    \n",
    "file_name = 'all_runs_{}.csv'\n",
    "#df.to_csv(file_name.format(event), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "415"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66662, 12)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
